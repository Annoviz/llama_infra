ARG BASE_IMAGE=ghcr.io/ggml-org/llama.cpp:full-cuda
FROM ${BASE_IMAGE}

ENV DEBIAN_FRONTEND=noninteractive

# Set CUDA_HOME, PATH and LD_LIBRARY_PATH
ENV CUDA_HOME /usr/local/cuda
ENV PATH /usr/local/cuda/bin:$PATH
ENV LD_LIBRARY_PATH /usr/local/cuda/lib64:$LD_LIBRARY_PATH

RUN nvcc --version

# Install Python
#RUN apt-get update && apt-get install -y python3 python3-pip python3-dev python3-venv

# Install Python packages
ARH REQUIREMENTS_FILE=requirements.txt
COPY $REQUIREMENTS_FILE /tmp/requirements.txt
RUN python3 -m pip install -r /tmp/requirements.txt

RUN apt-get update && apt-get install -y cmake git --no-install-recommends
ARG LLAMA_CPP_VERSION=0.3.7
ARG CMAKE_CUDA_ARCHITECTURES=70;75;80;86
RUN CMAKE_ARGS="-DGGML_CUDA=on -DCMAKE_CUDA_ARCHITECTURES=$CMAKE_CUDA_ARCHITECTURES" python3 -m pip install "llama-cpp-python[server]=$LLAMA_CPP_VERSION"


# Clean up
RUN apt-get clean && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*

# Clean up python packages
RUN rm -rf /root/.cache/pip

# Set the working directory
WORKDIR /app

# Set the entrypoint
COPY entrypoint.sh /app/entrypoint.sh
ENTRYPOINT ["/app/entrypoint.sh"]
#CMD ["python3", "server.py"]